{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twitter_gender.csv.zip',\n",
       " '.DS_Store',\n",
       " 'maleOrFemale.ipynb',\n",
       " 'botNotBot.ipynb',\n",
       " '02.CNN_100x50x300D_google_0.9693.h5',\n",
       " 'matrixTweetsEmb_ALT.dump',\n",
       " 'matrixTweetsEmb_FAST2.dump',\n",
       " 'google_w2v_300.bin',\n",
       " 'botNotBot.py',\n",
       " 'botNotBot_es.ipynb',\n",
       " 'gender-classifier-DFE-791531.csv',\n",
       " '01.weights.04-0.48041_0.8479.fasttext_female_male.hdf5',\n",
       " '02.CNN_100x50x300D_google_0.98.h5',\n",
       " 'createMaleFemale_new_examples.py',\n",
       " 'Dataset-README',\n",
       " 'botOrNot_loadAndFIlter.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " '01.CNN_100x50x300D_google_0.964.h5',\n",
       " 'en',\n",
       " '.git',\n",
       " 'listaClasses.dump',\n",
       " 'maleOrFemalet_loadAndFIlter.ipynb',\n",
       " '.idea']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathEn = \"/Users/kram/Downloads/botOrNot-en_es/es\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we will create a procedure to be tested on a single file.\n",
    "\n",
    "After this first step will be completed, we will extend this procedure to create a complete dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = \"/Users/kram/Downloads/botOrNot-en_es/es/1abac92163c7c4a410dad56c8feb7f18.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import io\n",
    "\n",
    "def iter_docs(author):\n",
    "    '''This function extracts the text and the language from the XML'''\n",
    "    author_attr = author.attrib\n",
    "    for doc in author.iter('document'):\n",
    "        doc_dict = author_attr.copy()\n",
    "        doc_dict.update(doc.attrib)\n",
    "        doc_dict['data'] = doc.text\n",
    "        yield doc_dict\n",
    "\n",
    "xml_data = open(testfile, \"r\") # Opening the text file\n",
    "etree = ET.parse(xml_data) # Create an ElementTree object \n",
    "df = pd.DataFrame(list(iter_docs(etree.getroot()))) #Append the info to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>√âxito de p√∫blico en la Zona de Estands de Empr...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üìÑüì£ Oferta de #empleo de un/a responsable de pr...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mapcesible. ¬°Haz visible lo accesible! App col...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>üìÑüì£ Oferta de #empleo de Cocineros/as en Montel...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sesi√≥n mon√≥grafica \"Capacidad de planificaci√≥n...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>üìÑüì£ Oferta de #empleo de \"Contable con experien...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>üìÑüì£ Oferta de #empleo de Cocineros/as en Arahal...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bases de la convocatoria de 4‚É£ plazas de polic...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.@websierranevada abre proceso de selecci√≥n (d...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>üìÑüì£ Oferta de #empleo de Cuidadores/as de perso...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data lang\n",
       "0  √âxito de p√∫blico en la Zona de Estands de Empr...   es\n",
       "1  üìÑüì£ Oferta de #empleo de un/a responsable de pr...   es\n",
       "2  Mapcesible. ¬°Haz visible lo accesible! App col...   es\n",
       "3  üìÑüì£ Oferta de #empleo de Cocineros/as en Montel...   es\n",
       "4  Sesi√≥n mon√≥grafica \"Capacidad de planificaci√≥n...   es\n",
       "5  üìÑüì£ Oferta de #empleo de \"Contable con experien...   es\n",
       "6  üìÑüì£ Oferta de #empleo de Cocineros/as en Arahal...   es\n",
       "7  Bases de la convocatoria de 4‚É£ plazas de polic...   es\n",
       "8  .@websierranevada abre proceso de selecci√≥n (d...   es\n",
       "9  üìÑüì£ Oferta de #empleo de Cuidadores/as de perso...   es"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'√âxito de p√∫blico en la Zona de Estands de Empresa de II Feria de Empleo @poligono_sur. Simult√°neamente se desarrolla el Taller Presenta tu Idea de Negocio y actividades relacionadas con Hosteler√≠a, Comercio, Log√≠stica y Servicios. #feps2018 #SevillaEmpleo #JornadasSE #EmprendeSE https://t.co/XGiyM1jaeU'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting ID to insert in the dataframe\n",
    "\n",
    "filename = testfile.split(\"/\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1abac92163c7c4a410dad56c8feb7f18'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try to extend the procedure to the full directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time is 39.54889392852783\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Creating empty dataframe\n",
    "dataEn = pd.DataFrame()\n",
    "\n",
    "# Monitoring time to load the files\n",
    "start = time.time()\n",
    "\n",
    "for root, dirs, files in os.walk(pathEn):\n",
    "    for file in files:\n",
    "        if file == 'truth.txt':\n",
    "            continue\n",
    "        else: \n",
    "            try:\n",
    "                pathToFile = root + '/' + file # Creating path\n",
    "                # print(pathToFile) # Just for debugging\n",
    "                xml_data = open(pathToFile, \"r\", encoding=\"utf8\") # Opening the text file\n",
    "                etree = ET.parse(xml_data) # Create an ElementTree object\n",
    "                data = list(iter_docs(etree.getroot())) # Create a list of dictionaries with the data\n",
    "                filename = file.split(\".\")[0] # Get filename\n",
    "                for dictionary in data: # Loop through the dictionary\n",
    "                    dictionary['ID'] = filename # Append filename\n",
    "                dataEn = dataEn.append(data)  # Append the list of dictionary to a pandas dataframe\n",
    "                \n",
    "            # If the file is not valid, skip it\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                continue\n",
    "            \n",
    "end = time.time()\n",
    "print(\"Total running time is\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>data</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>RT @Stoneismynamee: Si me quer√©is ganar en San...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>RT @amor_animal015: Choco a√∫n no tiene adopci√≥...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>RT @yisucrist: cuando t enteras d k tu humano ...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>RT @A3Noticias: La t√©cnica del 'bodypainting' ...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>RT @yisucrist: stoi yorando https://t.co/0uSb1...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>@FrankieJesstein @IanVanXanten ay dios, tiene ...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>Just posted a photo https://t.co/zcoGd0JnAm</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>RT @A3Noticias: Una hembra de labrador color c...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>RT @Vertele: Daniel Guzm√°n, el que faltaba por...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>RT @adoptaunavidaMD: KIMI, este zorro blanco, ...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ID  \\\n",
       "0  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "1  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "2  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "3  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "4  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "5  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "6  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "7  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "8  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "9  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "\n",
       "                                                data lang  \n",
       "0  RT @Stoneismynamee: Si me quer√©is ganar en San...   es  \n",
       "1  RT @amor_animal015: Choco a√∫n no tiene adopci√≥...   es  \n",
       "2  RT @yisucrist: cuando t enteras d k tu humano ...   es  \n",
       "3  RT @A3Noticias: La t√©cnica del 'bodypainting' ...   es  \n",
       "4  RT @yisucrist: stoi yorando https://t.co/0uSb1...   es  \n",
       "5  @FrankieJesstein @IanVanXanten ay dios, tiene ...   es  \n",
       "6        Just posted a photo https://t.co/zcoGd0JnAm   es  \n",
       "7  RT @A3Noticias: Una hembra de labrador color c...   es  \n",
       "8  RT @Vertele: Daniel Guzm√°n, el que faltaba por...   es  \n",
       "9  RT @adoptaunavidaMD: KIMI, este zorro blanco, ...   es  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataEn.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    RT @Stoneismynamee: Si me quer√©is ganar en San...\n",
       "0              \"Nunca podr√© mirarte y no sentir nada.\"\n",
       "0    EL OBSERVADOR : Diego Poyet, el uruguayo que n...\n",
       "0    RT @TVN: Blanca, la reina de las lateras en es...\n",
       "0    RT @BooktubeFIL: @dianajmorales Menos mal, cu√≠...\n",
       "0    El Congreso brasile√±o vuelve a salvar a Temer ...\n",
       "0    RT @luisscamposs: \"No Podr√© Vivir Sin Ti\" es l...\n",
       "0             MG: me caes de diez, sos re divertida ü§óüòÅ\n",
       "0                                          Psic√≥pata!!\n",
       "0    ‚ô• Es Q Yo Quiero Una Pisina ‚ô•\\n\\n:D Llena De r...\n",
       "0    @gerardo_staind obviamente m√°s joven de lo que...\n",
       "0                              https://t.co/YEfKgd9I7w\n",
       "0    Lo importante no es tener mucha gente a tus pi...\n",
       "0    Conozco a centenares de maridos que ser√≠an fel...\n",
       "0    @Elmeesmo @hijodevecino Es a la hora que termi...\n",
       "0    Combinar mi pijama de invierno con el de veran...\n",
       "0    Es importante formar parte de este tipo de ini...\n",
       "0                    Feliz con mi pijamita nueva. \\n‚Ä¶‚Ä¶\n",
       "0    #dc Kuala Lumpur y Singapur: dos ciudades, dos...\n",
       "0                                          siquitraque\n",
       "0    RT @VinelliU: ‚Äú¬øQui√©n es la se√±ora K? ¬°Keiko F...\n",
       "0    #es Detenidos dos hombres por una agresi√≥n hom...\n",
       "0                               los malditos y ellos:)\n",
       "0                                 @josepr1101 Jajaj no\n",
       "0    RT @SabuesoDigital_: ‚ñ∫ ¬°Ay, pap√°! Se destap√≥ u...\n",
       "0    #dc ‚ÄúLa derrota contra el Athletic no cambia n...\n",
       "0    ‚ÄúEl ejercicio m√°s noble de la mente y la mejor...\n",
       "0    RT @HogwartsTM: La cruda realidad. https://t.c...\n",
       "0    RT @conradhackett: This count doesn't include ...\n",
       "0    Con memes de Aniston, redes reaccionan a divor...\n",
       "                           ...                        \n",
       "0    Se precisa Encargado de T√∫nel. MUCHA EXPERIENC...\n",
       "0    RT @camilabressann: qu√© triste debe ser la vid...\n",
       "0    Sampaoli no deja de ser un excelente entrenado...\n",
       "0    Los Mossos encuentran fotos ‚Äúde car√°cter sexua...\n",
       "0    RT @leoagarciaa: A ver, @lopezobrador_  ya tra...\n",
       "0    Samsung Galaxy S3, se confirman cuatro colores...\n",
       "0    RT @roberuhp: Dir√°n que no es firme. Pueden re...\n",
       "0    EU y oposici√≥n cubana queremos lo mismo: Casa ...\n",
       "0    Vettel a Alonso: ‚Äú¬°Vaya idiota!‚Äù https://t.co/...\n",
       "0                                                00:00\n",
       "0    RT @hurgamemoriaPE: Si tiene tan buena recomen...\n",
       "0    RT @lucia_villalon: Gracias a todo el equipo d...\n",
       "0    el festival definitamnte se puso flayte #Vi√±a2...\n",
       "0      √âste s√≠ me hizo re√≠r... https://t.co/MTeWj8LqJp\n",
       "0                                     @RenzoPanfichi ü§ó\n",
       "0    @CruzadosSADP @JuanTagleQ Mis dudas de fin de ...\n",
       "0    Espectacular aurora polar en tiempo real http:...\n",
       "0    RT @TeLoCuentoNews: #Venezuela Alrededor de 10...\n",
       "0    RT @Izai_Zac: ‚ÄúEl Sistema Anticorrupci√≥n ampl√≠...\n",
       "0    #es Rep√∫blica Checa-Espa√±a en directo, el Euro...\n",
       "0    RT THIS! #AutoFollow #FollowNGain #InstantFoll...\n",
       "0    #eb Pablo Iglesias tilda de ‚Äúfraude‚Äù el intent...\n",
       "0    @marca me planteo seriamente si lo que os falt...\n",
       "0    Charla de Jose A. P√©rez, jueves 2 de octubre, ...\n",
       "0    RT @EncuestarTodo: #ParoFeminista\\n¬øBanc√°s el ...\n",
       "0    #Androide El primer Blackberry Android con lec...\n",
       "0    S.S.Dalai Lama \"El compromiso de practicar un ...\n",
       "0    RT @BugWarp: Lindas noticias de navidad. https...\n",
       "0    Katy Perry: Cabello azul. Nicki Minaj: Cabello...\n",
       "0    RT @Pucpto: A la chica que me le declar√© en te...\n",
       "Name: data, Length: 3000, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataEn['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>data</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3000</td>\n",
       "      <td>287626</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>5cae0202780b02d93a50911d02987855</td>\n",
       "      <td>La ciudad que solicitas esta mal escrita, o no...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>100</td>\n",
       "      <td>254</td>\n",
       "      <td>300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ID  \\\n",
       "count                             300000   \n",
       "unique                              3000   \n",
       "top     5cae0202780b02d93a50911d02987855   \n",
       "freq                                 100   \n",
       "\n",
       "                                                     data    lang  \n",
       "count                                              300000  300000  \n",
       "unique                                             287626       1  \n",
       "top     La ciudad que solicitas esta mal escrita, o no...      es  \n",
       "freq                                                  254  300000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataEn.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have merged the IDs with the data, we can create another dataframe with the labels and then merge them using the ID as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToLabels = \"/Users/kram/Downloads/botOrNot-en_es/es/truth.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kram/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "target = pd.read_csv(pathToLabels, sep=\":::\")\n",
    "target.columns=['ID', 'botOrHuman', 'sex'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>botOrHuman</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40adbb05f96fdd96f767b5967458faf1</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4aa2fb302140ec35cc6bc8a0d7d35f6</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58db587d884d22afefbcd37aa26af458</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd9494d1bff7fa477cc03fea5294a510</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ccf89c7b178b36d95d241c1c5f3da23</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b8ab53c54215915da53006d415aadc35</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9616ecddb159ad7ad99421903ac05499</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8b6826222306fd4694afbd709a1cb52c</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c75d850ee43e714116bf7c7b6a4f7793</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>d4f83ae6c529a0379bbf48fa02a5bbda</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ID botOrHuman  sex\n",
       "0  40adbb05f96fdd96f767b5967458faf1        bot  bot\n",
       "1   4aa2fb302140ec35cc6bc8a0d7d35f6        bot  bot\n",
       "2  58db587d884d22afefbcd37aa26af458        bot  bot\n",
       "3  dd9494d1bff7fa477cc03fea5294a510        bot  bot\n",
       "4  6ccf89c7b178b36d95d241c1c5f3da23        bot  bot\n",
       "5  b8ab53c54215915da53006d415aadc35        bot  bot\n",
       "6  9616ecddb159ad7ad99421903ac05499        bot  bot\n",
       "7  8b6826222306fd4694afbd709a1cb52c        bot  bot\n",
       "8  c75d850ee43e714116bf7c7b6a4f7793        bot  bot\n",
       "9  d4f83ae6c529a0379bbf48fa02a5bbda        bot  bot"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>botOrHuman</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2999</td>\n",
       "      <td>2999</td>\n",
       "      <td>2999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2999</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>82961d6f4353297b9e6445d547f2c103</td>\n",
       "      <td>human</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>1499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ID botOrHuman   sex\n",
       "count                               2999       2999  2999\n",
       "unique                              2999          2     3\n",
       "top     82961d6f4353297b9e6445d547f2c103      human   bot\n",
       "freq                                   1       1500  1499"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed with the concatenation of the dataframes for the English language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedEnData = pd.merge(dataEn, target, on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>data</th>\n",
       "      <th>lang</th>\n",
       "      <th>botOrHuman</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>RT @Stoneismynamee: Si me quer√©is ganar en San...</td>\n",
       "      <td>es</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>RT @amor_animal015: Choco a√∫n no tiene adopci√≥...</td>\n",
       "      <td>es</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>RT @yisucrist: cuando t enteras d k tu humano ...</td>\n",
       "      <td>es</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>RT @A3Noticias: La t√©cnica del 'bodypainting' ...</td>\n",
       "      <td>es</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>RT @yisucrist: stoi yorando https://t.co/0uSb1...</td>\n",
       "      <td>es</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>@FrankieJesstein @IanVanXanten ay dios, tiene ...</td>\n",
       "      <td>es</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>Just posted a photo https://t.co/zcoGd0JnAm</td>\n",
       "      <td>es</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>RT @A3Noticias: Una hembra de labrador color c...</td>\n",
       "      <td>es</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>RT @Vertele: Daniel Guzm√°n, el que faltaba por...</td>\n",
       "      <td>es</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3528ef91c7484ec189dcc82a7f9cd613</td>\n",
       "      <td>RT @adoptaunavidaMD: KIMI, este zorro blanco, ...</td>\n",
       "      <td>es</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ID  \\\n",
       "0  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "1  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "2  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "3  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "4  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "5  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "6  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "7  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "8  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "9  3528ef91c7484ec189dcc82a7f9cd613   \n",
       "\n",
       "                                                data lang botOrHuman     sex  \n",
       "0  RT @Stoneismynamee: Si me quer√©is ganar en San...   es      human  female  \n",
       "1  RT @amor_animal015: Choco a√∫n no tiene adopci√≥...   es      human  female  \n",
       "2  RT @yisucrist: cuando t enteras d k tu humano ...   es      human  female  \n",
       "3  RT @A3Noticias: La t√©cnica del 'bodypainting' ...   es      human  female  \n",
       "4  RT @yisucrist: stoi yorando https://t.co/0uSb1...   es      human  female  \n",
       "5  @FrankieJesstein @IanVanXanten ay dios, tiene ...   es      human  female  \n",
       "6        Just posted a photo https://t.co/zcoGd0JnAm   es      human  female  \n",
       "7  RT @A3Noticias: Una hembra de labrador color c...   es      human  female  \n",
       "8  RT @Vertele: Daniel Guzm√°n, el que faltaba por...   es      human  female  \n",
       "9  RT @adoptaunavidaMD: KIMI, este zorro blanco, ...   es      human  female  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergedEnData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>data</th>\n",
       "      <th>lang</th>\n",
       "      <th>botOrHuman</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>299900</td>\n",
       "      <td>299900</td>\n",
       "      <td>299900</td>\n",
       "      <td>299900</td>\n",
       "      <td>299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2999</td>\n",
       "      <td>287533</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>5cae0202780b02d93a50911d02987855</td>\n",
       "      <td>La ciudad que solicitas esta mal escrita, o no...</td>\n",
       "      <td>es</td>\n",
       "      <td>human</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>100</td>\n",
       "      <td>254</td>\n",
       "      <td>299900</td>\n",
       "      <td>150000</td>\n",
       "      <td>149900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ID  \\\n",
       "count                             299900   \n",
       "unique                              2999   \n",
       "top     5cae0202780b02d93a50911d02987855   \n",
       "freq                                 100   \n",
       "\n",
       "                                                     data    lang botOrHuman  \\\n",
       "count                                              299900  299900     299900   \n",
       "unique                                             287533       1          2   \n",
       "top     La ciudad que solicitas esta mal escrita, o no...      es      human   \n",
       "freq                                                  254  299900     150000   \n",
       "\n",
       "           sex  \n",
       "count   299900  \n",
       "unique       3  \n",
       "top        bot  \n",
       "freq    149900  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergedEnData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - CNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999\n"
     ]
    }
   ],
   "source": [
    "'''Creo la litsa degli ID, delle classi e dei tweets pr ogni ID'''\n",
    "\n",
    "listaIds =[]\n",
    "listaClasses = []\n",
    "matrixTweets = []\n",
    "\n",
    "for index, x in mergedEnData.iterrows():\n",
    "    id = x['ID']\n",
    "    if id not in listaIds:\n",
    "        newList = list()\n",
    "        newList.append(x[1])\n",
    "        matrixTweets.append(newList)\n",
    "        listaIds.append(id)\n",
    "        listaClasses.append(x[3])\n",
    "    else:\n",
    "        ls = matrixTweets[listaIds.index(id)]\n",
    "        ls.append(x[1])\n",
    "        matrixTweets[listaIds.index(id)] = ls\n",
    "        \n",
    "print(len(listaIds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trasformo le entit√†, lascio le faccine, levo le stopword e se serve agli embeddings lemmatizzo'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Trasformo le entit√†, lascio le faccine, levo le stopword e se serve agli embeddings lemmatizzo'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ekphrasis in /Users/kram/anaconda3/lib/python3.6/site-packages (0.5.1)\n",
      "Requirement already satisfied: nltk in /Users/kram/anaconda3/lib/python3.6/site-packages (from ekphrasis) (3.3)\n",
      "Requirement already satisfied: colorama in /Users/kram/anaconda3/lib/python3.6/site-packages (from ekphrasis) (0.3.9)\n",
      "Requirement already satisfied: matplotlib in /Users/kram/anaconda3/lib/python3.6/site-packages (from ekphrasis) (3.0.2)\n",
      "Requirement already satisfied: ftfy in /Users/kram/anaconda3/lib/python3.6/site-packages (from ekphrasis) (5.5.1)\n",
      "Requirement already satisfied: tqdm in /Users/kram/anaconda3/lib/python3.6/site-packages (from ekphrasis) (4.29.1)\n",
      "Requirement already satisfied: numpy in /Users/kram/anaconda3/lib/python3.6/site-packages (from ekphrasis) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /Users/kram/anaconda3/lib/python3.6/site-packages (from ekphrasis) (1.1.0)\n",
      "Requirement already satisfied: ujson in /Users/kram/anaconda3/lib/python3.6/site-packages (from ekphrasis) (1.35)\n",
      "Requirement already satisfied: six in /Users/kram/anaconda3/lib/python3.6/site-packages (from nltk->ekphrasis) (1.11.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/kram/anaconda3/lib/python3.6/site-packages (from matplotlib->ekphrasis) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/kram/anaconda3/lib/python3.6/site-packages (from matplotlib->ekphrasis) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/kram/anaconda3/lib/python3.6/site-packages (from matplotlib->ekphrasis) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/kram/anaconda3/lib/python3.6/site-packages (from matplotlib->ekphrasis) (2.7.3)\n",
      "Requirement already satisfied: wcwidth in /Users/kram/anaconda3/lib/python3.6/site-packages (from ftfy->ekphrasis) (0.1.7)\n",
      "Requirement already satisfied: setuptools in /Users/kram/anaconda3/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->ekphrasis) (39.1.0)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install ekphrasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "\n",
    "text_processor = TextPreProcessor (\n",
    "    # terms that will be normalized\n",
    "    normalize=[ 'email' , 'percent' , 'money' , 'phone' ,\n",
    "                'time' , 'url' , 'date' , 'number' ] ,\n",
    "    fix_html=True ,  # fix HTML tokens\n",
    "    segmenter=\"twitter\" ,\n",
    "    corrector=\"twitter\" ,\n",
    "    unpack_hashtags=True ,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=False ,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False ,  # spell correction for elongated words\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    dicts=[ emoticons ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transform sentences to word embeddings'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Transform sentences to word embeddings'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_300 = gensim.models.KeyedVectors.load_word2vec_format(\"/Volumes/MacPassport/PycharmProjects/botOrNot/cc.es.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.193 ,  0.0671, -0.1025, -0.0043, -0.0578, -0.0262, -0.0144,\n",
       "        0.0758,  0.1261,  0.0507,  0.0365,  0.1986,  0.033 ,  0.0436,\n",
       "       -0.0349,  0.031 ,  0.1231,  0.1072,  0.0074,  0.0073, -0.125 ,\n",
       "        0.126 , -0.0836,  0.0045,  0.0791, -0.059 , -0.0481, -0.0628,\n",
       "        0.1016,  0.0496,  0.0739, -0.019 , -0.1634, -0.0049,  0.0283,\n",
       "        0.0117,  0.0478, -0.0133,  0.0729,  0.0964, -0.1133,  0.0164,\n",
       "       -0.0526, -0.0978, -0.073 , -0.136 , -0.0006,  0.017 , -0.0392,\n",
       "        0.0585,  0.0285, -0.1074,  0.0173,  0.0581, -0.0737,  0.0148,\n",
       "       -0.0711,  0.0095,  0.1541, -0.0286, -0.0072,  0.0019, -0.0197,\n",
       "       -0.0128,  0.0457,  0.0399,  0.1521,  0.1045,  0.1238, -0.1073,\n",
       "        0.1641,  0.2329,  0.0276, -0.0535,  0.1128, -0.0268,  0.0432,\n",
       "       -0.3025,  0.0428, -0.0563,  0.1117, -0.0606, -0.0829,  0.1154,\n",
       "        0.04  ,  0.0205,  0.079 ,  0.0737,  0.1153, -0.0473,  0.075 ,\n",
       "        0.0078,  0.0061, -0.1011,  0.0308, -0.0921,  0.0359, -0.0455,\n",
       "       -0.0514,  0.0402, -0.0822,  0.002 , -0.031 ,  0.0383,  0.0642,\n",
       "       -0.0217,  0.1092,  0.0312,  0.0111,  0.251 ,  0.0306, -0.0013,\n",
       "       -0.0693, -0.0568, -0.1923,  0.0183,  0.0756, -0.0258,  0.0675,\n",
       "       -0.1169,  0.0476,  0.0378, -0.1087, -0.1699, -0.004 ,  0.1294,\n",
       "        0.0485, -0.118 ,  0.0332, -0.018 , -0.0361,  0.1387,  0.0233,\n",
       "        0.1008, -0.0016, -0.0797, -0.0031,  0.0718,  0.1686, -0.032 ,\n",
       "       -0.0057, -0.0024,  0.0666,  0.0579, -0.1503, -0.0891,  0.0237,\n",
       "       -0.0769, -0.0964, -0.1078, -0.0291, -0.0064, -0.0363,  0.1638,\n",
       "        0.0149,  0.0008,  0.0453,  0.1754,  0.0718,  0.1074, -0.1386,\n",
       "       -0.1048,  0.2158, -0.0723, -0.029 , -0.0869, -0.0215,  0.0366,\n",
       "       -0.0525, -0.0212, -0.0236,  0.0385,  0.0601, -0.0334,  0.0874,\n",
       "        0.1269, -0.0548, -0.0461,  0.0189,  0.1404,  0.0505, -0.0769,\n",
       "        0.0288, -0.0536, -0.1367,  0.087 ,  0.0251, -0.0379, -0.1244,\n",
       "       -0.0465,  0.0986,  0.0589,  0.0232, -0.0147, -0.174 ,  0.1006,\n",
       "        0.007 , -0.01  ,  0.0184,  0.054 , -0.0237,  0.0097, -0.1617,\n",
       "        0.052 , -0.0655,  0.2054, -0.1414, -0.1874, -0.0283,  0.2312,\n",
       "       -0.0321, -0.1105, -0.0791,  0.052 ,  0.1213, -0.0672, -0.1316,\n",
       "       -0.0334,  0.1096,  0.0585, -0.0698,  0.0397, -0.0621,  0.0206,\n",
       "       -0.0925, -0.016 ,  0.0827,  0.0731, -0.0305,  0.0347, -0.1221,\n",
       "       -0.1628,  0.0968,  0.0098,  0.0377,  0.0985, -0.0031, -0.0277,\n",
       "       -0.0669, -0.0045, -0.047 ,  0.1293,  0.021 , -0.1126, -0.023 ,\n",
       "       -0.0456, -0.0021,  0.1566, -0.0269, -0.1674,  0.1006, -0.0646,\n",
       "        0.1143, -0.0365, -0.0513, -0.0598,  0.027 ,  0.0522, -0.0045,\n",
       "       -0.1759, -0.1769, -0.0364, -0.0565, -0.1047, -0.0478, -0.0844,\n",
       "        0.0017,  0.1113,  0.0928, -0.0592, -0.0366, -0.052 , -0.1016,\n",
       "       -0.1097,  0.0621, -0.0489,  0.0668,  0.0268,  0.0982,  0.0671,\n",
       "       -0.0018,  0.1073,  0.0172, -0.2563,  0.1603,  0.1617, -0.0247,\n",
       "       -0.0396, -0.0122,  0.0472,  0.0219, -0.193 ,  0.0557, -0.0414,\n",
       "       -0.0433,  0.0312, -0.0082, -0.093 ,  0.0566, -0.0183],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Trasformo le frasi'''\n",
    "google_300.word_vec(\"link\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer as TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import random as rn\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "i = 0\n",
    "matrixTweetsEmb = []\n",
    "for tweetsUser in matrixTweets:\n",
    "    embTweetsUser = []\n",
    "    if(i % 100) == 0:\n",
    "         print(i)\n",
    "    for tweet in tweetsUser:\n",
    "        embTweetUser = np.zeros([50,300])\n",
    "        #Preprocesso\n",
    "        tokList = text_processor.pre_process_doc(tweet)\n",
    "        #Rimuovo le stopwords\n",
    "        tokList = [w for w in tokList if not w in stop_words]\n",
    "        #trovo l'embedding\n",
    "        numTok = 0;\n",
    "        for token in tokList[0:50]:\n",
    "            g_vec =[]\n",
    "            is_in_model = False\n",
    "            if token in google_300.vocab.keys ( ):\n",
    "                is_in_model = True\n",
    "                g_vec = google_300.word_vec(token)\n",
    "            elif token == \"<number>\":\n",
    "                is_in_model = True\n",
    "                g_vec = google_300.word_vec( \"n√∫mero\")\n",
    "            elif token == \"<percent>\":\n",
    "                is_in_model = True\n",
    "                g_vec = google_300.word_vec(\"porcentaje\")\n",
    "            elif token == \"<money>\":\n",
    "                is_in_model = True\n",
    "                g_vec = google_300.word_vec(\"dinero\")\n",
    "            elif token == \"<email>\":\n",
    "                is_in_model = True\n",
    "                g_vec = google_300.word_vec(\"email\")\n",
    "            elif token == \"<phone>\":\n",
    "                is_in_model = True\n",
    "                g_vec = google_300.word_vec(\"tel√©fono\")\n",
    "            elif token == \"<time>\":\n",
    "                is_in_model = True\n",
    "                g_vec = google_300.word_vec(\"hora\")\n",
    "            elif token == \"<date>\":\n",
    "                is_in_model = True\n",
    "                g_vec = google_300.word_vec(\"fecha\")\n",
    "            elif token == \"<url>\":\n",
    "                is_in_model = True\n",
    "                g_vec = google_300.word_vec(\"link\")\n",
    "            elif not is_in_model:\n",
    "                max = len ( google_300.vocab.keys ( ) ) - 1\n",
    "                index = rn.randint ( 0 , max )\n",
    "                word = google_300.index2word[ index ]\n",
    "                g_vec = google_300.word_vec( word )\n",
    "\n",
    "            embTweetUser[numTok] = np.array(g_vec)\n",
    "            numTok += 1\n",
    "        embTweetsUser.append(np.array(embTweetUser))\n",
    "    i +=1\n",
    "    matrixTweetsEmb.append(np.array(embTweetsUser))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2999, 100, 50, 300)\n"
     ]
    }
   ],
   "source": [
    "'''Num Utenti x Num Tweets x Num MaxTokens x Dim Embedding'''\n",
    "import numpy as np \n",
    "matrixTweetsEmb = np.array(matrixTweetsEmb)\n",
    "print(matrixTweetsEmb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['listaClasses.dump']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!{sys.executable} -m pip install joblib\n",
    "import joblib\n",
    "joblib.dump(matrixTweetsEmb,'/Volumes/MacPassport/PycharmProjects/BOToRNOT/matrixTweetsEmb_FAST.dump')\n",
    "#matrixTweetsEmb = joblib.load('matrixTweetsEmb_4177_100_50_300.dump')\n",
    "joblib.dump(listaClasses,'/Volumes/MacPassport/PycharmProjects/BOToRNOT/listaClasses.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kram/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 96, 46, 200)       1500200   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 48, 23, 200)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 44, 20, 100)       400100    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 22, 10, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 8, 20)         18020     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 4, 20)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 400)               320400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 2,339,222\n",
      "Trainable params: 2,339,222\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Conv2D(200,(5,5), activation ='relu', input_shape=(100,50,300)))\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Conv2D(100,(5,4), activation ='relu'))\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Conv2D(20,(3,3), activation ='relu'))\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(400, activation=\"tanh\"))\n",
    "model.add(Dense(200, activation=\"tanh\"))\n",
    "model.add(Dense(100, activation=\"tanh\"))\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'col': 0, 'mapping': [('human', 1), ('bot', 2)]}]\n"
     ]
    }
   ],
   "source": [
    "#!{sys.executable} -m pip install category_encoders\n",
    "import category_encoders as ce\n",
    "le =  ce.OneHotEncoder(return_df=False, impute_missing=False, handle_unknown=\"ignore\")\n",
    "training_classes = le.fit_transform(listaClasses)\n",
    "print(le.category_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile ( loss='categorical_crossentropy' , optimizer='adam' , metrics=['accuracy'] )\n",
    "\n",
    "history = model.fit(matrixTweetsEmb,training_classes,128,15,\n",
    "                      validation_split= 0.10 ,\n",
    "                      callbacks=callbacks_list,\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-28a4fbdfc0bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'qt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# summarize history for accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history.history.keys())\n",
    "%matplotlib qt\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('02.CNN_100x50x300D_google_0.9693.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(matrixTweetsEmb,listaClasses, test_size=0.10, random_state=891)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.metrics  import classification_report\n",
    "from keras.callbacks import Callback\n",
    "class MyCallBack(Callback):\n",
    "    def __init__(self,verbose=0):\n",
    "\n",
    "        super(Callback, self).__init__()\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get('val_loss')\n",
    "        # if current < 0.014:\n",
    "        #     self.model.stop_training = True\n",
    "\n",
    "        predicted = model.predict ( X_test )\n",
    "\n",
    "        test = [ '0' ] * len ( X_test )\n",
    "        i = 0\n",
    "        for cl in predicted:\n",
    "            test[ i ] = str ( np.argmax ( cl ) )\n",
    "            i += 1\n",
    "\n",
    "        test_lab = [ '0' ] * len ( X_test )\n",
    "        i = 0\n",
    "        for cl in y_test:\n",
    "            test_lab[ i ] = str ( np.argmax ( cl ) )\n",
    "            i += 1\n",
    "\n",
    "        print ( len ( X_test ) )\n",
    "        print ( classification_report ( test , test_lab, digits=5 ) )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2699 samples, validate on 300 samples\n",
      "Epoch 1/7\n",
      "2699/2699 [==============================] - 1409s 522ms/step - loss: 0.4780 - acc: 0.7736 - val_loss: 0.2934 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.86667, saving model to /Volumes/MacPassport/PycharmProjects/botOrNot/weights.01-0.29337-0.86667_fasttext_esp_hb.hdf5\n",
      "300\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.85401   0.85401   0.85401       137\n",
      "          1    0.87730   0.87730   0.87730       163\n",
      "\n",
      "avg / total    0.86667   0.86667   0.86667       300\n",
      "\n",
      "Epoch 2/7\n",
      "2699/2699 [==============================] - 1287s 477ms/step - loss: 0.2277 - acc: 0.9137 - val_loss: 0.2065 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.86667 to 0.91667, saving model to /Volumes/MacPassport/PycharmProjects/botOrNot/weights.02-0.20653-0.91667_fasttext_esp_hb.hdf5\n",
      "300\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.91241   0.90580   0.90909       138\n",
      "          1    0.92025   0.92593   0.92308       162\n",
      "\n",
      "avg / total    0.91664   0.91667   0.91664       300\n",
      "\n",
      "Epoch 3/7\n",
      "2699/2699 [==============================] - 1419s 526ms/step - loss: 0.1549 - acc: 0.9455 - val_loss: 0.1876 - val_acc: 0.9367\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.91667 to 0.93667, saving model to /Volumes/MacPassport/PycharmProjects/botOrNot/weights.03-0.18759-0.93667_fasttext_esp_hb.hdf5\n",
      "300\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.98540   0.88816   0.93426       152\n",
      "          1    0.89571   0.98649   0.93891       148\n",
      "\n",
      "avg / total    0.94115   0.93667   0.93655       300\n",
      "\n",
      "Epoch 4/7\n",
      "2699/2699 [==============================] - 1479s 548ms/step - loss: 0.1132 - acc: 0.9622 - val_loss: 0.2075 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.93667\n",
      "300\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.82482   0.94958   0.88281       119\n",
      "          1    0.96319   0.86740   0.91279       181\n",
      "\n",
      "avg / total    0.90830   0.90000   0.90090       300\n",
      "\n",
      "Epoch 5/7\n",
      "2699/2699 [==============================] - 1438s 533ms/step - loss: 0.0700 - acc: 0.9770 - val_loss: 0.2536 - val_acc: 0.9300\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.93667\n",
      "300\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    1.00000   0.86709   0.92881       158\n",
      "          1    0.87117   1.00000   0.93115       142\n",
      "\n",
      "avg / total    0.93902   0.93000   0.92992       300\n",
      "\n",
      "Epoch 6/7\n",
      "1472/2699 [===============>..............] - ETA: 10:15 - loss: 0.0610 - acc: 0.9735"
     ]
    }
   ],
   "source": [
    "model.compile ( loss='categorical_crossentropy' , optimizer='adam' , metrics=['accuracy'] )\n",
    "\n",
    "history = model.fit(X_train,y_train,batch_size=64,epochs=7,\n",
    "                      validation_data= (X_test,y_test) ,\n",
    "                      callbacks=callbacks_list,\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "test = ['0']* len(X_test)\n",
    "i= 0\n",
    "for cl in predicted:\n",
    "    test[i] = str(np.argmax (cl))\n",
    "    i +=1\n",
    "\n",
    "test_lab = ['0']* len(X_test)\n",
    "i= 0\n",
    "for cl in y_test:\n",
    "    test_lab[ i ] = str(np.argmax ( cl ))\n",
    "    i += 1\n",
    "\n",
    "\n",
    "print(len(X_test))\n",
    "acc = accuracy_score(test, test_lab)\n",
    "print(\" Accuracy:\", acc)\n",
    "\n",
    "print(classification_report(test,test_lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(matrixTweetsEmb,training_classes, test_size=0.10, random_state=891)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrixTweetsEmb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=\"/Volumes/MacPassport/PycharmProjects/botOrNot/weights.{epoch:02d}-{val_loss:.5f}-{val_acc:.5f}_fasttext_esp_hb.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    checkpoint,\n",
    "    MyCallBack(verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=7654).split(X_train,ytt))\n",
    "X_tr = []\n",
    "y_tr = []\n",
    "X_te = []\n",
    "y_te = []\n",
    "for j , (train_idx , val_idx) in enumerate ( folds ):\n",
    "    print ( '\\nFold ' , j )\n",
    "    X_tr = X_train[ train_idx ]\n",
    "    y_tr = y_train[ train_idx ]\n",
    "    X_te = X_train[ val_idx ]\n",
    "    y_te = y_train[ val_idx ]\n",
    "    y_tr = ce.transform(y_tr)\n",
    "    y_te = ce.transform(y_te)\n",
    "    model.fit(X_tr,y_tr,64,15,\n",
    "                          validation_data= (X_te,y_te) ,\n",
    "                          callbacks=callbacks_list,\n",
    "                          verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('human', 1), ('bot', 2)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.category_mapping[0]['mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytt = []\n",
    "for i in y_train:\n",
    "    if np.argmax(i) == 0:\n",
    "        cl = 'human'\n",
    "        ytt.append(cl)\n",
    "    else:\n",
    "        cl = 'bot'\n",
    "        ytt.append(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       ...,\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = le.transform(y_test)\n",
    "y_train = le.transform(y_train)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
